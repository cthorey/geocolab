{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named geocolab.Data_Utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f1e755c6abf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'autoreload 2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgeocolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData_Utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named geocolab.Data_Utils"
     ]
    }
   ],
   "source": [
    "import os,re,sys\n",
    "sys.path.append('..')\n",
    "import time,pickle\n",
    "from tqdm import *\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from geocolab.Data_Utils import *\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint\n",
    "from stop_words import get_stop_words  # load stopwords\n",
    "import Stemmer  # Load an inplementation of the snow-ball stemmer\n",
    "\n",
    "model_saved = os.path.join('../data')\n",
    "abstractf = os.path.join(model_saved,'model_abstract','abstract')\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.tools import FigureFactory as FF \n",
    "plotly.offline.init_notebook_mode()  \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "American  Geophysical Union  (AGU) meeting  is a  geoscience conference\n",
    "hold  each year  around Christmas  in San  Francisco. It  represents a\n",
    "great opportunity for PhD students like  me to show off their work and\n",
    "enjoy what the west coast has to offer. However, with  nearly 24 000 attendees,  AGU Fall Meeting is  also the\n",
    "largest Earth  and space  science meeting  in the  world. As  such, it\n",
    "represents an interesting data set to dive into the geoscience academic\n",
    "world.  \n",
    "\n",
    "In this post, I explore different information retrieval techniques taken from the field of natural language processing to explore the hidden patterns in the submitted abstract collection in 2015.\n",
    "\n",
    "The objective is two fold:\n",
    "\n",
    "- Identify semantic-based similarities between the contribution proposed at AGU to build a recommandation system based on the abstract content.\n",
    "- Propose for each contributor a list of potential collaborators based on the authors of the papers proposed by our recommendantion system.\n",
    "\n",
    "Different natural language processing tools are available in python to achieve this goal and after trying [sklearn](http://scikit-learn.org/stable/), I decided to settle on [gensim](https://radimrehurek.com/gensim/) which has partilarly fast and effective implementations to work with large dataset (~20000 abstracts here).\n",
    "\n",
    "The basic stage, which I'll detail in the following are\n",
    "\n",
    "- Cleaning the data.\n",
    "- Construct a valid embedding for the corpus.\n",
    "- Compute the similarities between the document within this embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning is an essential step for our recommendation system. Indeeed, our model is going to use the resulting corpus to build a consistent embedding of the abstracts and we don't want him to focus on unnecessary details. In particular, I used the module **unicodedata** to remove non-ascii caracters from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_data = os.path.join(model_saved,'data_agu2015')\n",
    "data = get_all_data(path_data)\n",
    "sources = [df for df in data if (''.join(df.title) != \"\") and (df.abstract != '') and (len(df.abstract.split(' '))>100)]\n",
    "sections = [df.section for df in sources]\n",
    "abstracts = get_clean_abstracts(sources)\n",
    "titles = get_clean_titles(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, I'll use on of my contributions to evaluate the consistency of our recommendation system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def name_to_idx(sources,name):\n",
    "    ''' From an authors, return the list of contributions '''\n",
    "    contrib = [f for f in sources if name in f.authors.keys()]\n",
    "    return [sources.index(elt) for elt in contrib]\n",
    "    \n",
    "my_contrib = name_to_idx(sources,'Clement Thorey')\n",
    "print 'Title : %s'%(titles[my_contrib[0]])\n",
    "print 'Abstract : %s'%(abstracts[my_contrib[0]])+'\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be a bit of context can be usefull here. My PhD was about the detection and the characterization of magmatic intrusions on terrestrial planets. For those who wonder, a magmatic intrusion is a large volume of magma which, instead of rising until the surface and form a volcano, emplace at depth beneath the surface (less than a few km) where it cools and solidifies. On Earth, erosion and weathering can sometimes expose these intrusions at the surface. This is the case for instance in the henry mountains\n",
    "\n",
    "![Example of an exposed magmatic intrusion in the Henry Mountains](https://upload.wikimedia.org/wikipedia/commons/a/a6/Laccolith_Montana.jpg)\n",
    "\n",
    "My contributions at AGU deals with the detection and the characterization of those intrusions.\n",
    "\n",
    "The first one is about the detection of a specific family of magmatic intrusions on the Moon which we call crater-centered intrusions. Particularly, those are magmatic intrusions that have emplaced and solidify beneath large impact craters (>20km in diameter) at the surface of the Moon. Consequently, these crater are heavily deformed due to the magmatic intrusion with large network of fracture crossing their floor.  In this contribution, I use machine learning techniques to try to automatically detect potential floor-fractured craters among 60000 referenced lunar impact craters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic representation for a corpus of text document is called a [Bag of Word (BoW) model](https://en.wikipedia.org/wiki/Bag-of-words_model). This model looks at all the words in the corpus and first build a dictionary referencing all the words it has seen. Then, for each document in the corpus, in simply count how many times each word of the dictionary appears in this particular document. The result is a large matrix, each row is a text document, each columns is a particular word of the dicitonarry, that is, as you can guess, mostly fill with zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the BoW model assume an efficient tokenizer function which is able to split each document it its own set of tokens. A vanilla tokenizer function looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which simply look at each document and split it in a list of tokens according to the white spaces in the document. In the following, I'll use a slightly more evolve version of this tokenizer which I embedded in a Tokenizer class.\n",
    "\n",
    "It use the [nltk](http://www.nltk.org/) library to first break each document (abstract) into sentences, then words.\n",
    "Then using reg expression, it keeps only suitable tokens. In particular,\n",
    "\n",
    "- `^[a-z]+$` keeps only words made of letters.\n",
    "- `^[a-z][\\d]$` selects tokens that have 2 characters, one letter, one number (molecule stuff).\n",
    "- `^[a-z][\\d][a-z]$` selects tokens that have 3 characters, one letter, one number, one letter (again molecule stuff).\n",
    "- `^[a-z]{3}[a-z]*-[a-z]*$` includes some tokens that are composed of two words joined by -.\n",
    "\n",
    "Next, I use a stopword list provide by **nltk** to filter out all the common word of the english language. Indeed, stopwords are words like 'the' or 'as' that are most likely present everywhere but do not carry meaningfull information in our purpose. This tokenizer also incorporates a last stage of stemming for each token. Stemming is the term used in information retrieval to describe the process for reducing words to their word stem, base or root form—generally a written word form. \n",
    "\n",
    "For instance, imagine this document\n",
    "\n",
    "'Here we show that running is good for health. Indeed runner are quite healthy. Though they have runned a lot in their runly life, they are quite good at that.'\n",
    "\n",
    "Clearly, this document is all about running! Nevertheless, without the stemming part in our tokenizer, 'runly' will have the same weight than 'good', equal to 1. In contrast, the stemming will reduce 'running', 'runned', 'runly' and 'runner' to their stem, namely 'run'. The word 'run' in the BoW will then have a weight of 4 for this document clearly underlying its importance ! I use the so-called SnowballStemmer included in the library **nltk** for stemming. \n",
    "\n",
    "Finally, in addition to these simple stem tokens, I also add the possibility to use bi-grams to the dictionary, i.e. all the combinations of two consecutive stem-words in each abstracts which is a common practise when using BoW model. We will see why later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    ''' Class to handle the tokenization of a document.\n",
    "\n",
    "    parameter:\n",
    "    add_bigram (Boolean): Add the possibility to add bigram to the resulting\n",
    "    list of tokens\n",
    "\n",
    "    '''\n",
    "    def __init__(self, add_bigram):\n",
    "        self.add_bigram = add_bigram\n",
    "        self.stopwords = get_stop_words('english')\n",
    "        self.stopwords += [u's', u't', u'can', u'will', u'just', u'don', u'now']\n",
    "        self.stemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "    def bigram(self, tokens):\n",
    "        if len(tokens) > 1:\n",
    "            for i in range(0, len(tokens) - 1):\n",
    "                yield tokens[i] + '_' + tokens[i + 1]\n",
    "\n",
    "    def tokenize_and_stem(self, text):\n",
    "        tokens = list(gensim.utils.tokenize(text))\n",
    "        filtered_tokens = []\n",
    "        bad_tokens = []\n",
    "        # filter out any tokens not containing letters (e.g., numeric tokens, raw\n",
    "        # punctuation)\n",
    "        for token in tokens:\n",
    "            if re.search('(^[a-z]+$|^[a-z][\\d]$|^[a-z]\\d[a-z]$|^[a-z]{3}[a-z]*-[a-z]*$)', token):\n",
    "                filtered_tokens.append(token)\n",
    "            else:\n",
    "                bad_tokens.append(token)\n",
    "        filtered_tokens = [\n",
    "            token for token in filtered_tokens if token not in self.stopwords]\n",
    "        stems = map(self.stemmer.stemWord, filtered_tokens)\n",
    "        if self.add_bigram:\n",
    "            stems += [f for f in self.bigram(stems)]\n",
    "        return map(str, stems)\n",
    "    \n",
    "tokenizer = Tokenizer(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the next step is to build the dictionary. **Gensim** is built in a memory-friendly fashion. Therefore, instead of loading the whole corpus into memory, tokenizing and stemming everything and see what remains, it allows us to build the dictionary document by document, with one document in memory at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build = False\n",
    "if build:\n",
    "    # First, write the document corpus on a txt file, one document perline.\n",
    "    write_clean_corpus(abstracts,abstractf+'_data.txt')\n",
    "    tokeniser = Tokenizer(False)\n",
    "    # Next create the dictionary by iterating of the abstract, one per line in the txt file\n",
    "    dictionary = corpora.Dictionary(tokenizer.tokenize_and_stem(line) for line in open(abstractf+'_data.txt')) \n",
    "    dictionary.save(abstractf+'_raw.dict')\n",
    "else:\n",
    "    tokeniser = Tokenizer(False)\n",
    "    dictionary = corpora.Dictionary.load(abstractf+'_raw.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dictionary contains 70150 tokens. While we could work out a BoW model from there, it is often a good idea to remove extreme tokens. For instance, a token appearing in only 1 abstract is not going to help us build a recommandation system. Similarly, a token that appears in all the documents is not likely to carry meaningfull information neither for our purpose. I thereferore decided to remove all tokens that appear in less than 5 abstracts and in more than 80% of them. Note that creating the dictionarry can take up to 1 minute on my laptop which make serialization a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build = False\n",
    "if not os.path.isfile(abstractf+'_raw.dict') or build:\n",
    "    dictionary =  corpora.Dictionary.load(abstractf+'_raw.dict')\n",
    "    dictionary.filter_extremes(no_below=5,no_above=0.80,keep_n=200000)\n",
    "    dictionary.id2token = {k:v for v,k in dictionary.token2id.iteritems()}\n",
    "    dictionary.save(abstractf+'.dict')\n",
    "else:\n",
    "    dictionary = corpora.Dictionary.load(abstractf+'.dict')\n",
    "    dictionary.id2token = {k:v for v,k in dictionary.token2id.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the dictionary, it is actually easy to obtain the BoW representation of any document. We just have to tokenize the document using the same function used to build the dictionary and count the occurence of each word. Each dictionary in **gensim** possess a method **doc2bow** which does exactly that and return the representation as a sparse vector, i.e. a vector where only words that have a count different from zero are returned.\n",
    "\n",
    "For instance, the BoW representation of my first abstract is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_contrib_bow = dictionary.doc2bow(tokenizer.tokenize_and_stem(abstracts[my_contrib[0]]))\n",
    "df = [f+(dictionary.id2token[f[0]],) for f in my_contrib_bow]\n",
    "df = pd.DataFrame(df,columns = ['id','Count','Token']).sort_values(by='Count',ascending = False)\n",
    "df.index= range(len(df))\n",
    "table = FF.create_table(df.head(5))\n",
    "#py.iplot(table, filename='Bow_0')\n",
    "plotly.offline.iplot(table, show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the result are presented as a pandas dataframe for clarity and each id has been identified to its proper token. Indeed, each dictionary assign a unique integer id to all tokens appearing in the dictionary. Note that the BoW representation of my first abstract on Floor-Fractured craters, which underlies the importance of the stem token crater, lunar, intrusion, floor and classifi, is farely accurate.\n",
    "\n",
    "By converting each abstract of the corpus in this doc2bow method, we can obtain the BoW representation of our full corpus. A careless memory way to do that is to just iterate the doc2bow method of our dictionary over the abstract list we have defined at the beginning. Nevertheless, this would end up storing the whole doc2bow representation into memory as a huge matrice. Instead, **gensim** has been designed such that it only requires that a corpus must be able to return one document vector (for instance, the doc2bow representation of the document here) at a time. We then define the BoW corpus as a sprecific object `MyCorpus` where the method `__iter__` is consistently defined to iter and transform each line of the txt file where the abstracts content is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(Tokenizer):\n",
    "\n",
    "    def __init__(self, name, add_bigram):\n",
    "        super(MyCorpus, self).__init__(add_bigram)\n",
    "        self.name = name\n",
    "        self.load_dict()\n",
    "\n",
    "    def load_dict(self):\n",
    "        if not os.path.isfile(self.name + '.dict'):\n",
    "            print 'You should build the dictionary first !'\n",
    "        else:\n",
    "            setattr(self, 'dictionary',\n",
    "                    corpora.Dictionary.load(self.name + '.dict'))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.name + '_data.txt'):\n",
    "            # assume there's one document per line, tokens separated by\n",
    "            # whitespace\n",
    "            yield self.dictionary.doc2bow(self.tokenize_and_stem(line))\n",
    "\n",
    "    def __str__(self, n):\n",
    "        for i, line in enumerate(open(self.name + '_data.txt')):\n",
    "            print line\n",
    "            if i > n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build = False\n",
    "if build:\n",
    "    bow_corpus = MyCorpus(abstractf,False)\n",
    "    corpora.MmCorpus.serialize(abstractf+'_bow.mm',bow_corpus)\n",
    "else:\n",
    "    bow_corpus = corpora.MmCorpus(abstractf+'_bow.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MyCorpus` also posseses a print method which return the BoW representation of the first n document. Again, the return representation is parsed, i.e. it contains only the counts for non-zero element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_corpus.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the BoW representation of our corpus, each abstract is a point in a high-dimensional embedding (a 14669 dimensions embedding exactly). The *distance* or the *similarity* between one abstract and the rest of the corpus, according to some metrics, can then be used to compare different contributions together and then, to provide a recommendation list for a specific query. \n",
    "\n",
    "The euclidean distance is the more natural choice for the similarity measure. Given two vectors  $\\vec{a}$ and $\\vec{b}$, it is equal to \n",
    "$$d(\\vec{a},\\vec{b}) = \\sqrt{(\\vec{b}- \\vec{a})\\cdot(\\vec{b}- \\vec{a}) }$$\n",
    "\n",
    "\n",
    "However, we'd like our distance to be independant of the magnitude of the difference between two vectors. For instance, we'd like to identify as similar two abstracts which contain exactly the same tokens even if their occurence differs significantly. The euclidean distance clearly does not have this property.\n",
    "\n",
    "Accordingly, a more reliable measure for our purpose is called \"the cosine similarity\". For two vectors, $\\vec{a}$ and $\\vec{b}$, the cosine similarity $d$ is defined as :\n",
    "\n",
    "$$ d(\\vec{a},\\vec{b})= \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|} = \\cos(\\vec{a},\\vec{b})$$\n",
    "\n",
    "In particular, this similarity measure is the dot product of the two normalized vector and hence, depends only on the angle between the two vectors (which is were its name comes from ;). It ranges from -1 when two vectors point in the opposite direction to 1 when they point in the same direction.\n",
    "\n",
    "To compute the similarity of one query against our BoW representation, the natural procedure is to first transform our sparse representation into its dense equivalent, i.e. a matrice where the number of lines correspond to the number of tokens in the dictionary and the number of columns to the number of abstracts in the corpus. Then, we column normalize the matrice such that each document correspond to a unit vector in the representation space. Finaly, we take the dot product of the transposed matrice with the desired normalized query to get the cosine similarity agaist all documents in the corpus.\n",
    "\n",
    "**Gensim** contains efficient utility functions to help converting from/to numpy matrice and therefore, this translates to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recom(abstractf,name):\n",
    "    dictionary = corpora.Dictionary.load(abstractf+'.dict') \n",
    "    corpus = corpora.MmCorpus(abstractf + '_'+str(name)+'.mm')\n",
    "    index = similarities.MatrixSimilarity.load(abstractf+'_'+str(name)+'.index')\n",
    "    score = index[corpus[my_contrib[0]]]\n",
    "    results = pd.DataFrame(np.stack((np.sort(score)[::-1],np.array(titles)[np.argsort(score)[::-1]])).T,\n",
    "                       columns = ['CosineSimilarity','Title'])\n",
    "    return results\n",
    "\n",
    "df = recom(abstractf,'bow')\n",
    "for i,row in df.iterrows():\n",
    "    print 'Recom %d - Cosine: %1.3f - Title: %s'%(i+1,float(row.CosineSimilarity),row.Title)\n",
    "    if i>8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problem with the BoW representation is that it often puts too much weights on common words of the corpus. Indeed, while we remove most common words in english, i.e. the stopwords, word like 'present', 'show' of whatever is commonly use in the writing-abstract vocabulary can add some noise in regards to our purpose. In particular here, we would like to put more weights on tokens that make each abstract specific.\n",
    "\n",
    "A common way to do this is to use a **Tf-Idf** normalization to re-weiht each count in the BoW representation by the frequency of the token in the whole corpus. **Tf** means term-frequency while **Tf–Idf** means term-frequency times inverse document-frequency. This way, the weight of common tokens in the corpus will be significantly lowered.\n",
    "\n",
    "This implentation is available is **gensim** and can be easily combined with the BoW representation to get the representation of the corpus in the tf-idf space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build = False\n",
    "if not os.path.isfile(abstractf+'_tfidf.mm') or build:\n",
    "    # First load the corpus and the dicitonary\n",
    "    bow_corpus = corpora.MmCorpus(abstractf+'.mm')\n",
    "    dictionary = corpora.Dictionary.load(abstractf+'.dict')\n",
    "    # Initialize the tf-idf model\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    # Compute the tfidf of the corpus itself\n",
    "    tfidf_corpus = tfidf[bow_corpus]\n",
    "    # Serialize both for reuse\n",
    "    tfidf.save(abstractf+'_tfidf.model')\n",
    "    corpora.MmCorpus.serialize(abstractf+'_tfidf.mm',tfidf_corpus)\n",
    "else:\n",
    "    tfidf = models.TfidfModel.load(abstractf+'_tfidf.model')\n",
    "    tfidf_corpus = corpora.MmCorpus(abstractf+'_tfidf.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = recom(abstractf,'tfidf')\n",
    "for i,row in df.iterrows():\n",
    "    print 'Recom %d - Cosine: %1.3f - Title: %s'%(i+1,float(row.CosineSimilarity),row.Title)\n",
    "    if i>8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indeed produces a slight improve of the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA) or (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here comes  Latent Semantic Analysis (LSA) or  Indexing (LSI). LSI\n",
    "is a common method in information retrieval to reduce the dimension of\n",
    "the representation  space. The  idea behind  it is that  a lot  of the\n",
    "dimensions  in  the  previous   representations  are  redundant.   For\n",
    "instance,  the words  machine and  learning are  more likely  to occur\n",
    "together. Therefore, shrinking these two  dimensions to only one which\n",
    "is form  by a  linear combination  of the  token machine  and learning\n",
    "would  reduce the  dimension without  any loss  of information.   More\n",
    "generally, the Latent Semantic Analysis  aims to reduce the dimensions\n",
    "while  keeping as  much  information possible  present  in the  higher\n",
    "dimensal space by identifying deep semantic pattern in the corpus.\n",
    "\n",
    "To identify this  semantic structure, Latent Semantic  Analysis used a\n",
    "linear               algebra               method               called\n",
    "[Singular Value Decomposition (SVD)](https://en.wikipedia.org/wiki/Latent_semantic_analysis). \n",
    "More formally, we know that the tf-idf representation can be written as a huge matric $X$ where each line corresponds to a token in the dictionary and each column corresponds to a document. Its size is (T,N) where T is the number of tokens and N the number of abstracts.\n",
    "\n",
    "The maths behind LSI factorizes the matrice $X$ as\n",
    "\n",
    "$$ X = UDV^T $$\n",
    "\n",
    "where $U$ is a unitary matrix, i.e. formed by orthogonal unit norm vectors, of size (T,T), $D$ is a diagonal rectangular matrix of size (T,N) and $V$ is also a unitary matrix of size (N,N). The vectors in $U$ are called left eigenvectors, the vectors in $V$ are called right eigenvectors and the matrice $D$ is composed by their corresponding eigenvalues.\n",
    "\n",
    "Particularly, this factorization has a nice property regarding the matrice containing all the cross-documents dot-product $X^TX$. Indeed,\n",
    "\n",
    "$$X^TX = (UDV^T)^T(UDV^T) = (VD^TU^T)(UDV^T) = VD^TDV^T$$\n",
    "\n",
    "and therefore, the orthonormal vectors in $V$ can be seen as the eigenvectors of the cross-document correlation matrice $X^TX$. As D is diagonal, $D^TD$ is just composed by the eigenvalues squared and therefore, these eigenvalues can be used as a direct proxy to evaluate the variance in the cross-document correlation matrice.\n",
    "\n",
    "A rank $k$ approximation of $X$ can be obtain by\n",
    "\n",
    "$$X_k = U_kD_kV^T_k$$\n",
    "\n",
    "where $U_k$ and $V_k$ are the matrices $U$ and $V$ where we kept only the $k$ first eigenvectors, i.e size (T,K) and (N,K) respectively and $D_k$ is a squared matrice of size (K,K) which contains the k first eigenvalues of the diagonal. More importanly, $V_kD^T_k$ gives us a the new representation, called LSI space, where each document is characterized by k features. The   SVD  is  thus able  to   identify  a   consistent  lower-dimensional\n",
    "approximation  of  the   higher-dimensional  tfidf  space. \n",
    "\n",
    "**Gensim**\n",
    "implements  the   Latent  Semantic  Analysis  under   a  model  called\n",
    "`LsiModel` which  can be  used on top  of our  previous representation\n",
    "easily. It  requires a parameter, **num_topics**,  which corresponds to\n",
    "the  desired   dimension  in  the   final  lsi  space.  I   settle  on\n",
    "**num_topics=500** for good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build = False\n",
    "if not os.path.isfile(abstractf+'_lsi.mm') or build:\n",
    "    # First load the corpus and the dicitonary\n",
    "    tfidf_corpus = corpora.MmCorpus(abstractf+'_tfidf.mm')\n",
    "    dictionary = corpora.Dictionary.load(abstractf+'.dict')\n",
    "    # Initialize the LSI model\n",
    "    lsi = models.LsiModel(tfidf_corpus,id2word=dictionary, num_topics=500)\n",
    "    # Compute the tfidf of the corpus itself\n",
    "    lsi_corpus = lsi[tfidf_corpus]\n",
    "    # Serialize both for reuse\n",
    "    lsi.save(abstractf+'_lsi.model')\n",
    "    corpora.MmCorpus.serialize(abstractf+'_lsi.mm',lsi_corpus)\n",
    "else:\n",
    "    lsi = models.LsiModel.load(abstractf+'_lsi.model')\n",
    "    lsi_corpus = corpora.MmCorpus(abstractf+'_lsi.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we used $k=500$ here. The matrice $U$ and $D$ can be easily extracted from the model and we verify that the return lsi_corpus correspond to $V_kD_k^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = gensim.matutils.corpus2dense(lsi_corpus, len(lsi.projection.s)).T / lsi.projection.s\n",
    "lsi_corpus_dense = gensim.matutils.corpus2dense(lsi_corpus,len(lsi.projection.s))\n",
    "np.allclose(np.dot(V,np.diag(lsi.projection.s)),lsi_corpus_dense.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create traces\n",
    "trace0 = go.Scatter(\n",
    "    x = range(len(lsi.projection.s)),\n",
    "    y = lsi.projection.s,\n",
    "    mode = 'markers+lines',\n",
    "    name = 'markers',\n",
    "    marker = {'size':10}\n",
    ")\n",
    "layout = go.Layout(\n",
    "    margin = {'b':30,'r':30,'l':30,'t':30},\n",
    "    title='Eigenvalues of the SVD factorization',\n",
    "    legend = {'yanchor':'auto',\n",
    "              'bgcolor':'#EAEAF2',\n",
    "              'xanchor':'auto',\n",
    "             'font':{'size':9}})\n",
    "\n",
    "data = [trace0]\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "# Plot and embed in ipython notebook!\n",
    "plotly.offline.iplot(fig, show_link=False)\n",
    "#py.iplot(fig, filename='scatter-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_n_topics(n,k):\n",
    "    k -=1\n",
    "    words = map(lambda x:dictionary.id2token[x],list(np.argsort(lsi.projection.u[:,k])[::-1])[:10])\n",
    "    coeff = ['%1.3f'%(f) for f in list(np.sort(lsi.projection.u[:,k])[::-1])[:10]]\n",
    "    return reduce(lambda x,y: y+' + '+x, [g+'x'+f for f,g in zip(words,coeff)][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_n_topics(10,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.show_topics(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = recom(abstractf,'lsi')\n",
    "for i,row in df.iterrows():\n",
    "    print 'Recom %d - Cosine: %1.3f - Title: %s'%(i+1,float(row.CosineSimilarity),row.Title)\n",
    "    if i>8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-sne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named palettable.tableau",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-75d56c2f53af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpalettable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtableau\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTableau_20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstractf\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_Xtsne.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0munique_section\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTableau_20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex_colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msection2color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named palettable.tableau"
     ]
    }
   ],
   "source": [
    "from palettable.tableau import Tableau_20\n",
    "X_tsne = joblib.load(abstractf+'_Xtsne.pkl')\n",
    "unique_section = set(sections)\n",
    "color = Tableau_20.hex_colors\n",
    "section2color = dict(zip(set(sections),color))\n",
    "labels = np.array(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color = Tableau_20.hex_colors\n",
    "from palettable.colorbrewer.sequential import BuPu_7\n",
    "clever_color = dict.fromkeys(unique_section)\n",
    "clever_color[u' SPA-Aeronomy']= sns.color_palette('Oranges').as_hex()[-2]\n",
    "clever_color[u' SPA-Magnetospheric Physics']= sns.color_palette('Oranges').as_hex()[-2]\n",
    "clever_color[u' SPA-Solar and Heliospheric Physics']= sns.color_palette('Oranges').as_hex()[-2]\n",
    "clever_color[u' Planetary Sciences']= sns.color_palette('Oranges').as_hex()[-1]\n",
    "clever_color[u' Geodesy']= sns.color_palette('Oranges').as_hex()[-3]\n",
    "clever_color[u' Geomagnetism and Paleomagnetism']= sns.color_palette('Oranges').as_hex()[-4]\n",
    "clever_color[u' Atmospheric Sciences']= sns.color_palette('Reds').as_hex()[-1]\n",
    "clever_color[u' Atmospheric and Space Electricity']= sns.color_palette('Reds').as_hex()[-2]\n",
    "clever_color[u' Cryosphere']= sns.color_palette('Blues').as_hex()[-1]\n",
    "clever_color[u' Ocean Sciences']= sns.color_palette('Blues').as_hex()[-3]\n",
    "clever_color[u' Hydrology']= sns.color_palette('Blues').as_hex()[-2]\n",
    "\n",
    "clever_color[u' Biogeosciences']= sns.color_palette('Greens').as_hex()[-1]\n",
    "\n",
    "clever_color[u' Earth and Planetary Surface Processes']= BuPu_7.hex_colors[-1]\n",
    "clever_color[u' Natural Hazards']= BuPu_7.hex_colors[-2]\n",
    "clever_color[u' Seismology']= BuPu_7.hex_colors[-3]\n",
    "clever_color[u' Tectonophysics']= BuPu_7.hex_colors[-3]\n",
    "clever_color[u' Volcanology, Geochemistry and Petrology']= BuPu_7.hex_colors[-4]\n",
    "\n",
    "clever_color[u' Near Surface Geophysics']= Tableau_20.hex_colors[-3]\n",
    "clever_color[u' Mineral and Rock Physics']= Tableau_20.hex_colors[-4]\n",
    "clever_color[u\" Study of Earth's Deep Interior\"]= Tableau_20.hex_colors[-4]\n",
    "\n",
    "clever_color[u' Nonlinear Geophysics']= Tableau_20.hex_colors[10]\n",
    "clever_color[u' Earth and Space Science Informatics']= Tableau_20.hex_colors[11]\n",
    "\n",
    "clever_color[u' Education']= Tableau_20.hex_colors[-5]\n",
    "clever_color[u' Public Affairs']= Tableau_20.hex_colors[-5]\n",
    "clever_color[u' Union']= Tableau_20.hex_colors[-5]\n",
    "section2color = clever_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_section= [u' SPA-Aeronomy',u' SPA-Magnetospheric Physics',u' SPA-Solar and Heliospheric Physics',u' Planetary Sciences',\n",
    "                u' Geodesy',u' Geomagnetism and Paleomagnetism',u' Atmospheric Sciences',u' Atmospheric and Space Electricity',\n",
    "                u' Atmospheric and Space Electricity',u' Cryosphere',u' Ocean Sciences',u' Hydrology',u' Biogeosciences',\n",
    "                u' Earth and Planetary Surface Processes',u' Natural Hazards',u' Seismology',u' Tectonophysics',\n",
    "                u' Volcanology, Geochemistry and Petrology',u' Near Surface Geophysics',u' Mineral and Rock Physics',\n",
    "                u\" Study of Earth's Deep Interior\",u' Nonlinear Geophysics',u' Earth and Space Science Informatics',\n",
    "                u' Education',u' Public Affairs',u' Union']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traces = []\n",
    "for section in tqdm(unique_section,total = len(unique_section)):\n",
    "    mask = labels == section\n",
    "    x_data = X_tsne[mask,0]\n",
    "    y_data = X_tsne[mask,1]\n",
    "    text = np.array(titles)[mask] \n",
    "\n",
    "    trace = go.Scattergl(\n",
    "        x = x_data,\n",
    "        y = y_data,\n",
    "        text = [f[:50]+'...' for f in text],\n",
    "        name = section,\n",
    "        hoverinfo = 'name+text',\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 5,\n",
    "            opacity = 0.5,\n",
    "            color = section2color[section],\n",
    "            line = {'width':0}\n",
    "        )\n",
    "        )\n",
    "    traces.append(trace)\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis = {'showline':False,\n",
    "             'showticklabels':False,\n",
    "             'zerolinewidth':0,\n",
    "             'showgrid':False,\n",
    "            'domain':[0,0.7]},\n",
    "    yaxis = {'showline':False,\n",
    "            'showticklabels':False,\n",
    "            'zerolinewidth':0,\n",
    "            'showgrid':False},\n",
    "    height = 600,\n",
    "    width = 900,\n",
    "    margin = {'b':30,'r':30,'l':30,'t':30},\n",
    "    title='t-sne of the 500 LSA embedding',\n",
    "    legend = {'yanchor':'auto',\n",
    "              'bgcolor':'#EAEAF2',\n",
    "              'xanchor':'auto',\n",
    "             'font':{'size':9}})\n",
    "\n",
    "data = traces\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "#plotly.offline.iplot(fig, show_link=False)\n",
    "py.iplot(fig,filename = 'Tsne_LSA_AGU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recom(query):\n",
    "    ''' Final recommendation system \n",
    "    \n",
    "    Input:\n",
    "    - abstractf : path where to load the dictionary,lsi corpus, and index corpus.\n",
    "    \n",
    "    ''' \n",
    "    # Load the necessary models, the lsi corpus and the correpsonding index\n",
    "    dictionary = corpora.Dictionary.load(abstractf+'.dict') \n",
    "    tfidf = models.TfidfModel.load(abstractf+'_tfidf.model')\n",
    "    lsi = models.LsiModel.load(abstractf+'_lsi.model')\n",
    "    corpus = corpora.MmCorpus(abstractf + '_lsi.mm')\n",
    "    index = similarities.MatrixSimilarity.load(abstractf+'_lsi.index')\n",
    "    \n",
    "    #Transform the query in lsi space\n",
    "    ## Transform in the bow representation space\n",
    "    vec_bow = dictionary.doc2bow(tokeniser.tokenize_and_stem(query))\n",
    "    ## Transform in the tfidf representation space\n",
    "    vec_tfidf = tfidf[vec_bow]\n",
    "    ## Transform in the lsi representation space\n",
    "    vec_lsi = lsi[vec_tfidf]\n",
    "    ## Get the cosine siminalrity of the query agaisnt all the abstracts\n",
    "    cosine = index[vec_lsi]\n",
    "    ## Sort them and return a nice dataframe\n",
    "    results = pd.DataFrame(np.stack((np.sort(cosine)[::-1],np.array(titles)[np.argsort(cosine)[::-1]])).T,\n",
    "                       columns = ['CosineSimilarity','Title'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Collaborators search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_data = os.path.join(model_saved,'data_agu2015')\n",
    "annuary = get_all_contrib(path_data)\n",
    "index_authors = pd.DataFrame([[f.name,f.country,f.link] for f in annuary],columns = ['name','country','link'])\n",
    "idx_authors = index_authors.groupby('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary which map each title to another dictionary\n",
    "# Containing the different absract contributors together with som\n",
    "# useful information\n",
    "\n",
    "def get_country(name):\n",
    "    try: \n",
    "        country = idx_authors.get_group(name).country.values[0].upper()\n",
    "    except:\n",
    "        country = ''\n",
    "    return country\n",
    "authors = {titles[i]:{key:\n",
    "                      {'inst':inst,\n",
    "                       'link':obj.link,\n",
    "                       'title':titles[i],\n",
    "                       'country':get_country(key)}\n",
    "                      for key, inst in obj.authors.iteritems()} for i,obj in enumerate(sources)}\n",
    "def collab_based_on_n_abstract(query,n = 5):\n",
    "    ''' Return a list of the potential contributors based\n",
    "    on the n first abstract proposed by the recommendation \n",
    "    system '''\n",
    "    df = recom(query)\n",
    "    df = df.head(n)\n",
    "    collab = {}\n",
    "    for i,row in df.iterrows():\n",
    "        collab.update(authors[row.Title])\n",
    "    for name,description in collab.iteritems():\n",
    "        print '%s from the %s, %s'%(name.upper(),description['inst'],idx_authors.get_group(name).country.values[0].upper())\n",
    "        print 'Based on his/her abstract untitled'\n",
    "        print ' %s'%(description['title'])\n",
    "        print '%s \\n'%(description['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collab_based_on_n_abstract(abstracts[my_contrib[0]],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
